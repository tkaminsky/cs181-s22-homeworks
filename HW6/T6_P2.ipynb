{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS 181 HW6 Problem 2 - Policy and Value Iteration**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util methods to represent the grid (do not modify)\n",
    "#### You do not need modify any of these methods to complete Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maze state is represented as a 2-element NumPy array: (Y, X). Increasing Y is South.\n",
    "grid = [\n",
    "    'o.x.u',\n",
    "    '..y..',\n",
    "    '..y.*',\n",
    "    '..z..']\n",
    "topology = np.array([list(row) for row in grid])\n",
    "flat_topology = topology.ravel()\n",
    "rewards = {'.': 0, '*': 50, 'o': 4, 'u': 20, 'x': -10, 'y': -50,'z': -20}\n",
    "\n",
    "row_count = len(grid) # 4\n",
    "col_count = len(grid[0]) # 5\n",
    "shape = (row_count, col_count) # (4, 5)\n",
    "\n",
    "# Possible actions, expressed as (delta-y, delta-x)\n",
    "directions=\"NSEW\"\n",
    "maze_actions = {\n",
    "    'N': np.array([-1, 0]),\n",
    "    'S': np.array([1, 0]),\n",
    "    'E': np.array([0, 1]),\n",
    "    'W': np.array([0, -1]),\n",
    "}\n",
    "actions = [maze_actions[direction] for direction in directions]\n",
    "\n",
    "\n",
    "# Returns true if pos (y,x) is out of bounds\n",
    "def is_wall(pos):\n",
    "    (y, x) = pos\n",
    "    return (y < 0 or y >= row_count or x < 0 or x >= col_count)\n",
    "\n",
    "# Input is a flattened state, returns the reward at that state\n",
    "def get_reward(state):\n",
    "    assert (state in range(num_states)), f\"get_reward: State was not an integer representing an in-bounds state ({state} was given)\"\n",
    "    return rewards.get(flat_topology[int(state)])\n",
    "\n",
    "# Input is a flattened state, returns the unflattened representation of the state\n",
    "def unflatten_index(flattened_index):\n",
    "    return np.unravel_index(flattened_index, shape)\n",
    "\n",
    "# Input state is an unflattened position and action is an index into the actions[] array\n",
    "# Returns a tuple containing the new position of taking the action from the state\n",
    "def move(state, action):\n",
    "    return tuple((state + actions[action]).reshape(1, -1)[0])\n",
    "\n",
    "# Returns an array of the \"side states\" when taking action beginning at unflattened position state\n",
    "# Does not return states which are out of bounds\n",
    "def get_side_states(action, state):\n",
    "    side_states = []\n",
    "    \n",
    "    if action == 0 or action == 1:\n",
    "        if not is_wall(move(state, 3)):\n",
    "            side_states.append(move(state, 3))\n",
    "        if not is_wall(move(state, 2)):\n",
    "            side_states.append(move(state, 2))\n",
    "    elif action == 2 or action == 3:\n",
    "        if not is_wall(move(state, 0)):\n",
    "            side_states.append(move(state, 0))\n",
    "        if not is_wall(move(state, 1)):\n",
    "            side_states.append(move(state, 1))\n",
    "            \n",
    "    return side_states\n",
    "    \n",
    "# Inputs s1, s2 are flattened states, action represents an index into the actions array\n",
    "# Returns p(s2 | s1, action)\n",
    "def get_transition_prob(s1, action, s2):\n",
    "    # Check the inputs are valid\n",
    "    assert (action in [0,1,2,3]), f\"get_transition_prob: Action needs to be an integer in [0,1,2,3], but {action} was given\"\n",
    "    assert (s1 in range(num_states)), f\"get_transition_prob: Input s1 was not an integer representing an in-bounds state ({s1} was given)\"\n",
    "    assert (s2 in range(num_states)), f\"get_transition_prob: Input s2 was not an integer representing an in-bounds state ({s2} was given)\"\n",
    "    \n",
    "    state1 = unflatten_index(int(s1))\n",
    "    state2 = unflatten_index(int(s2))\n",
    "    action = int(action)\n",
    "\n",
    "    new_state = move(state1, action)\n",
    "\n",
    "    sstates = get_side_states(action, state1)\n",
    "    succeed_prb = 0.8\n",
    "    slip_prb = 0.1\n",
    "\n",
    "    # One of the side states was a wall: adjust probabilities accordingly.\n",
    "    if len(sstates) == 1:\n",
    "        succeed_prb = 0.9\n",
    "\n",
    "    if is_wall(new_state):\n",
    "        if(state1 == state2):\n",
    "            return succeed_prb\n",
    "    else:\n",
    "        if(state2 == new_state):\n",
    "            return succeed_prb\n",
    "\n",
    "    # Oherwise, check if state2 is on either side\n",
    "    for side_state in sstates:\n",
    "        if(state2 == side_state):\n",
    "            return slip_prb\n",
    "\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE PLOTS (do not modify)\n",
    "\n",
    "# Util to draw the value function V as numbers on a plot.\n",
    "def make_value_plot(V):\n",
    "    # Useful stats for the plot\n",
    "    value_function = np.reshape(V, shape)\n",
    "\n",
    "    # Write the value on top of each square\n",
    "    indx, indy = np.arange(row_count), np.arange(col_count)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(value_function, interpolation='none', cmap= plt.get_cmap('coolwarm_r'))\n",
    "\n",
    "    for s in range(len(V)):\n",
    "        val = V[s]\n",
    "        (xval, yval) = unflatten_index(s)\n",
    "        t = \"%.2f\"%(val,) # format value with 1 decimal point\n",
    "        ax.text(yval, xval, t, color='black', va='center', ha='center', size=15)\n",
    "        \n",
    "\n",
    "# Util to draw the policy pi as arrows on a plot.\n",
    "def make_policy_plot(pi, iter_type, iter_num):\n",
    "    # Useful stats for the plot\n",
    "    row_count = len(grid)\n",
    "    col_count = len(grid[0])\n",
    "    policy_function = np.reshape(pi, shape)\n",
    "\n",
    "    for row in range(row_count):\n",
    "        for col in range(col_count):\n",
    "            if policy_function[row,col] == 0:\n",
    "                dx = 0; dy = -.5\n",
    "            if policy_function[row,col] == 1:\n",
    "                dx = 0; dy = .5\n",
    "            if policy_function[row,col] == 2:\n",
    "                dx = .5; dy = 0\n",
    "            if policy_function[row,col] == 3:\n",
    "                dx = -.5; dy = 0\n",
    "            plt.arrow( col , row , dx , dy , shape='full', fc='w' , ec='gray' , lw=1., length_includes_head=True, head_width=.1 )\n",
    "    plt.title(iter_type + ' Iteration, i = ' + str(iter_num))\n",
    "    # plt.savefig(iter_type + '_' + str(iter_num) + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please read the rest of this file before beginning to code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(181)\n",
    "VALUE_ITER = 'Value'\n",
    "POLICY_ITER = 'Policy'\n",
    "\n",
    "num_states = shape[0] * shape[1] # num_states = 20\n",
    "num_actions = len(actions) # num_actions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DOS FOR PROBLEM 2\n",
    "Below you will implement policy and value iteration.\n",
    "\n",
    "A state is represented as an integer from ``0`` to ``num_states - 1``\n",
    "<br>\n",
    "An action is represented as an integer in ``[0,1,2,3]``, which represents the four cardinal directions [N,S,E,W]\n",
    "\n",
    "Each state has a reward associated with it. The agent gains the reward of a state when it takes an action at that state, not immediately upon entry.\n",
    "\n",
    "``pi`` contains the learned policy at each state, represented by an array of length ``num_states``. In this exercise we will be implementing a deterministic policy, so each state has exactly one action associated with it.\n",
    "<br>\n",
    "ex: [2, 3, 1, 1, 2, 0, 1, 2, 2, 1, 3, 0, 0, 2, 2, 1, 3, 3, 2, 0]\n",
    "\n",
    "``V`` represents the learned value function at each state. Like the above, it is also represented as an array of length ``num_states`` where the entry at index ``i`` represents the value of state ``i``.\n",
    "\n",
    "\n",
    "## Helper methods\n",
    "\n",
    "Recall that when you take an action in Gridworld, you won't always necessarily move in that direction. Instead there is some probability of moving to a state on either side. You do not need to calculate these transition probabilities yourself. Please use the helper functions ``get_transition_prob`` and ``get_reward`` in this file. The method headers are listed below:\n",
    "\n",
    "``get_reward(state):`` Input is a state, output is the reward at that state\n",
    "\n",
    "``get_transition_prob(s1, a, s2):`` Returns the probability of transitioning from state ``s1`` to state ``s2`` upon taking action ``a``.\n",
    "\n",
    "An example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "print(get_reward(14))\n",
    "print(get_transition_prob(16, 0, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) Policy Evaluation\n",
    "\n",
    "Returns array ``V`` representing the value of policy ``pi`` using discount factor ``gamma``\n",
    "\n",
    "Note: You can do this either closed-form or iteratively. If performed iteratively, please use a convergence tolerance of at least ``0.0001``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this function\n",
    "def policy_evaluation(pi, gamma):\n",
    "    theta = 0.0001\n",
    "    V = np.zeros(num_states)\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) Policy Iteration\n",
    "\n",
    "Now that we have ``V`` computed in 1a), perform **one step** of policy iteration to return the updated policy ``pi_new``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this function\n",
    "def update_policy_iteration(V, gamma):\n",
    "    pi_new = np.zeros(num_states)\n",
    "\n",
    "    return pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Value Iteration\n",
    "Given a value function ``V`` and a policy ``pi``, perform **one step** of value iteration and return the updated ``V_new``, ``pi_new``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this function\n",
    "def update_value_iteration(V, pi, gamma):\n",
    "    V_new = np.zeros(num_states)\n",
    "    pi_new = np.zeros(num_states)\n",
    "\n",
    "    return V_new, pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run code, plot results\n",
    "\n",
    "The ``learn_strategy`` method iteratively runs the one-step methods you wrote in parts 1 and 2, either until the value function converges under <TT>ct</TT> or until a specified number of iterations <TT>max_iter</TT> have elapsed. It will also print out intermediate plots of the learned policy and value function at intervals of <TT>print_every</TT>. The arguments of the function are listed in more detail below:\n",
    "\n",
    "``planning_type`` (<TT>string</TT>): \n",
    "    Specifies whether value or policy iteration is used to learn the strategy.\n",
    "    \n",
    "``max_iter`` (<TT>int</TT>):\n",
    "    The maximum number of iterations (i.e. number of updates) the learning\n",
    "    policy will be run for.\n",
    "    \n",
    "``print_every`` (<TT>int</TT>):\n",
    "    The frequency at which the function will print value and policy plots.\n",
    "    \n",
    "``ct`` (<TT>float</TT>):\n",
    "    The convergence tolerance used for policy or value iteration.\n",
    "    \n",
    "``gamma`` (<TT>float</TT>):\n",
    "    The discount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the learn_strategy method, but read through its code\n",
    "def learn_strategy(planning_type = VALUE_ITER, max_iter = 10, print_every = 5, ct = None, gamma = 0.7):\n",
    "    # Loop over some number of episodes\n",
    "    V = np.zeros(num_states)\n",
    "    pi = np.zeros(num_states)\n",
    "\n",
    "    # Update Q-table using value/policy iteration until max iterations or until ct reached\n",
    "    for n_iter in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "\n",
    "        # Update V and pi using value or policy iteration.\n",
    "        if planning_type == VALUE_ITER:\n",
    "            V, pi = update_value_iteration(V, pi, gamma)\n",
    "        elif planning_type == POLICY_ITER:\n",
    "            V = policy_evaluation(pi, gamma)\n",
    "            pi = update_policy_iteration(V, gamma)\n",
    "        \n",
    "        # Calculate the difference between this V and the previous V\n",
    "        diff = np.absolute(np.subtract(V, V_prev))\n",
    "\n",
    "        # Check that every state's difference is less than the convergence tol\n",
    "        if ct and np.max(diff) < ct:\n",
    "            make_value_plot(V = V)\n",
    "            make_policy_plot(pi = pi, iter_type = planning_type, iter_num = n_iter+1)\n",
    "            print(\"Converged at iteration \" + str(n_iter+1))\n",
    "            return 0\n",
    "\n",
    "        # Make value plot and plot the policy\n",
    "        if (n_iter % print_every == 0):\n",
    "            make_value_plot(V = V)\n",
    "            make_policy_plot(pi = pi, iter_type = planning_type, iter_num = n_iter+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Beginning policy iteration.')\n",
    "learn_strategy(planning_type=POLICY_ITER, max_iter = 4, print_every = 1, ct = 0.01, gamma = 0.7)\n",
    "print('Policy iteration complete.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Beginning value iteration.')\n",
    "learn_strategy(planning_type=VALUE_ITER, max_iter = 4, print_every = 1, ct = 0.01, gamma = 0.7)\n",
    "print('Value iteration complete.\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
